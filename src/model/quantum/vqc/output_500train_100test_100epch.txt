/mnt/slurm_nfs/j32abrah/ece-733-quantum-ml/src/model/quantum/vqc/vqc_quantum.py:234: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.
  vqc_qnn = EstimatorQNN(

=== Train Distribution ===
        class_label  samples     pct
0            normal    67343  53.458
1           neptune    41214  32.717
2             satan     3633   2.884
3           ipsweep     3599   2.857
4         portsweep     2931   2.327
5             smurf     2646   2.100
6              nmap     1493   1.185
7              back      956   0.759
8          teardrop      892   0.708
9       warezclient      890   0.707
10              pod      201   0.160
11     guess_passwd       53   0.042
12  buffer_overflow       30   0.024
13      warezmaster       20   0.016
14             land       18   0.014
15             imap       11   0.009
16          rootkit       10   0.008
17       loadmodule        9   0.007
18        ftp_write        8   0.006
19         multihop        7   0.006
20              phf        4   0.003
21             perl        3   0.002
22              spy        2   0.002

=== Test Distribution ===
        class_label  samples     pct
0            normal     9711  43.076
1           neptune     4657  20.657
2      guess_passwd     1231   5.460
3             mscan      996   4.418
4       warezmaster      944   4.187
5           apache2      737   3.269
6             satan      735   3.260
7      processtable      685   3.039
8             smurf      665   2.950
9              back      359   1.592
10        snmpguess      331   1.468
11            saint      319   1.415
12         mailbomb      293   1.300
13    snmpgetattack      178   0.790
14        portsweep      157   0.696
15          ipsweep      141   0.625
16       httptunnel      133   0.590
17             nmap       73   0.324
18              pod       41   0.182
19  buffer_overflow       20   0.089
20         multihop       18   0.080
21            named       17   0.075
22               ps       15   0.067
23         sendmail       14   0.062
24            xterm       13   0.058
25          rootkit       13   0.058
26         teardrop       12   0.053
27            xlock        9   0.040
28             land        7   0.031
29           xsnoop        4   0.018
30        ftp_write        3   0.013
31       loadmodule        2   0.009
32             worm        2   0.009
33             perl        2   0.009
34        sqlattack        2   0.009
35         udpstorm        2   0.009
36              phf        2   0.009
37             imap        1   0.004

=== Train Distribution ===
    class_label  samples   pct
0        normal      267  53.4
1       neptune      164  32.8
2       ipsweep       14   2.8
3         satan       14   2.8
4     portsweep       12   2.4
5         smurf       10   2.0
6          nmap        6   1.2
7      teardrop        4   0.8
8   warezclient        4   0.8
9          back        4   0.8
10          pod        1   0.2

=== Test Distribution ===
      class_label  samples   pct
0          normal       45  45.0
1         neptune       21  21.0
2    guess_passwd        7   7.0
3           mscan        5   5.0
4     warezmaster        4   4.0
5           smurf        3   3.0
6           satan        3   3.0
7    processtable        3   3.0
8            nmap        2   2.0
9         apache2        2   2.0
10      snmpguess        2   2.0
11  snmpgetattack        1   1.0
12           back        1   1.0
13      portsweep        1   1.0
Train label counts:
 label_binary
0    267
1    233
Name: count, dtype: int64
Test label counts:
 label_binary
1    55
0    45
Name: count, dtype: int64
Processed shapes: X_train: (500, 101) X_test: (100, 101)
Class weights: {np.int64(0): np.float64(0.9363295880149812), np.int64(1): np.float64(1.0729613733905579)}
PCA fitted -> reduced to 2 dims and saved (pca_nsl_kdd.pkl).
Torch tensor shapes: X_train: torch.Size([500, 2]) X_test: torch.Size([100, 2])
Epoch 1/100 — Loss: 0.718446
Epoch 2/100 — Loss: 0.716691
Epoch 3/100 — Loss: 0.714962
Epoch 4/100 — Loss: 0.713264
Epoch 5/100 — Loss: 0.711452
Epoch 6/100 — Loss: 0.709561
Epoch 7/100 — Loss: 0.707634
Epoch 8/100 — Loss: 0.705529
Epoch 9/100 — Loss: 0.703265
Epoch 10/100 — Loss: 0.700635
Epoch 11/100 — Loss: 0.697966
Epoch 12/100 — Loss: 0.695169
Epoch 13/100 — Loss: 0.692326
Epoch 14/100 — Loss: 0.689209
Epoch 15/100 — Loss: 0.686091
Epoch 16/100 — Loss: 0.683082
Epoch 17/100 — Loss: 0.680024
Epoch 18/100 — Loss: 0.677030
Epoch 19/100 — Loss: 0.674257
Epoch 20/100 — Loss: 0.671359
Epoch 21/100 — Loss: 0.668717
Epoch 22/100 — Loss: 0.666202
Epoch 23/100 — Loss: 0.663813
Epoch 24/100 — Loss: 0.661468
Epoch 25/100 — Loss: 0.659440
Epoch 26/100 — Loss: 0.657259
Epoch 27/100 — Loss: 0.655341
Epoch 28/100 — Loss: 0.653586
Epoch 29/100 — Loss: 0.651871
Epoch 30/100 — Loss: 0.650217
Epoch 31/100 — Loss: 0.648663
Epoch 32/100 — Loss: 0.647176
Epoch 33/100 — Loss: 0.645682
Epoch 34/100 — Loss: 0.644391
Epoch 35/100 — Loss: 0.642998
Epoch 36/100 — Loss: 0.641767
Epoch 37/100 — Loss: 0.640481
Epoch 38/100 — Loss: 0.639260
Epoch 39/100 — Loss: 0.638134
Epoch 40/100 — Loss: 0.636978
Epoch 41/100 — Loss: 0.635918
Epoch 42/100 — Loss: 0.634895
Epoch 43/100 — Loss: 0.633771
Epoch 44/100 — Loss: 0.632766
Epoch 45/100 — Loss: 0.631797
Epoch 46/100 — Loss: 0.630828
Epoch 47/100 — Loss: 0.629930
Epoch 48/100 — Loss: 0.628988
Epoch 49/100 — Loss: 0.628158
Epoch 50/100 — Loss: 0.627348
Epoch 51/100 — Loss: 0.626456
Epoch 52/100 — Loss: 0.625604
Epoch 53/100 — Loss: 0.624799
Epoch 54/100 — Loss: 0.623984
Epoch 55/100 — Loss: 0.623267
Epoch 56/100 — Loss: 0.622450
Epoch 57/100 — Loss: 0.621679
Epoch 58/100 — Loss: 0.620879
Epoch 59/100 — Loss: 0.620130
Epoch 60/100 — Loss: 0.619443
Epoch 61/100 — Loss: 0.618734
Epoch 62/100 — Loss: 0.617992
Epoch 63/100 — Loss: 0.617238
Epoch 64/100 — Loss: 0.616564
Epoch 65/100 — Loss: 0.615893
Epoch 66/100 — Loss: 0.615190
Epoch 67/100 — Loss: 0.614524
Epoch 68/100 — Loss: 0.613787
Epoch 69/100 — Loss: 0.613241
Epoch 70/100 — Loss: 0.612468
Epoch 71/100 — Loss: 0.611879
Epoch 72/100 — Loss: 0.611149
Epoch 73/100 — Loss: 0.610486
Epoch 74/100 — Loss: 0.609901
Epoch 75/100 — Loss: 0.609234
Epoch 76/100 — Loss: 0.608533
Epoch 77/100 — Loss: 0.607905
Epoch 78/100 — Loss: 0.607324
Epoch 79/100 — Loss: 0.606631
Epoch 80/100 — Loss: 0.605966
Epoch 81/100 — Loss: 0.605366
Epoch 82/100 — Loss: 0.604744
Epoch 83/100 — Loss: 0.604093
Epoch 84/100 — Loss: 0.603470
Epoch 85/100 — Loss: 0.602889
Epoch 86/100 — Loss: 0.602243
Epoch 87/100 — Loss: 0.601706
Epoch 88/100 — Loss: 0.601024
Epoch 89/100 — Loss: 0.600359
Epoch 90/100 — Loss: 0.599830
Epoch 91/100 — Loss: 0.599216
Epoch 92/100 — Loss: 0.598563
Epoch 93/100 — Loss: 0.597879
Epoch 94/100 — Loss: 0.597313
Epoch 95/100 — Loss: 0.596723
Epoch 96/100 — Loss: 0.596112
Epoch 97/100 — Loss: 0.595534
Epoch 98/100 — Loss: 0.595056
Epoch 99/100 — Loss: 0.594430
Epoch 100/100 — Loss: 0.593773
Training time: 614.49 seconds
Logits: tensor([-0.9280,  0.3379, -0.6503, -0.5688, -0.4004,  0.6023,  0.1922, -0.3418,
        -1.2272, -1.2126, -0.2894, -1.1959,  0.3429, -0.2233, -0.1600,  0.4076,
        -0.3650, -0.3099,  0.6413, -0.3721, -1.2299, -0.5641, -1.2278, -0.6039,
         0.5559,  0.0406, -0.7247, -0.5120, -0.2724,  0.2423, -0.3692,  0.5595,
        -0.2311,  0.5886, -0.7335, -0.6054, -0.9011,  0.3014,  0.8988, -0.7651,
         0.6285, -0.3175,  0.0576, -1.2366, -0.1680, -0.8080, -0.8909, -0.1613,
         0.7333, -0.8949, -0.8911, -0.0363,  0.0311, -0.8302, -0.4453, -1.1892,
        -0.2667, -0.8635, -0.9402, -0.2136, -0.3288, -0.4034, -0.0801,  0.3173,
         0.5942,  0.0438,  0.0105, -0.0371, -0.5951, -0.0311, -0.0083, -0.4240,
        -0.1884, -0.7884, -0.3663, -0.7355,  0.0317, -0.1901, -0.7104,  0.3342,
         0.9648,  0.5721, -0.1629, -0.5653,  0.5758, -0.7994, -0.2321,  0.2748,
         0.6615,  0.3363, -0.9365, -0.9212, -0.5541,  0.0158,  0.3287,  0.8474,
        -0.6696, -0.3055, -0.2351, -0.0769])
Probabilities: tensor([0.2833, 0.5837, 0.3429, 0.3615, 0.4012, 0.6462, 0.5479, 0.4154, 0.2267,
        0.2292, 0.4282, 0.2322, 0.5849, 0.4444, 0.4601, 0.6005, 0.4097, 0.4231,
        0.6550, 0.4080, 0.2262, 0.3626, 0.2266, 0.3534, 0.6355, 0.5102, 0.3263,
        0.3747, 0.4323, 0.5603, 0.4087, 0.6363, 0.4425, 0.6431, 0.3244, 0.3531,
        0.2888, 0.5748, 0.7107, 0.3175, 0.6521, 0.4213, 0.5144, 0.2250, 0.4581,
        0.3083, 0.2909, 0.4598, 0.6755, 0.2901, 0.2909, 0.4909, 0.5078, 0.3036,
        0.3905, 0.2334, 0.4337, 0.2966, 0.2809, 0.4468, 0.4185, 0.4005, 0.4800,
        0.5787, 0.6443, 0.5109, 0.5026, 0.4907, 0.3555, 0.4922, 0.4979, 0.3956,
        0.4530, 0.3125, 0.4094, 0.3240, 0.5079, 0.4526, 0.3295, 0.5828, 0.7241,
        0.6392, 0.4594, 0.3623, 0.6401, 0.3102, 0.4422, 0.5683, 0.6596, 0.5833,
        0.2816, 0.2847, 0.3649, 0.5039, 0.5814, 0.7000, 0.3386, 0.4242, 0.4415,
        0.4808])
Predictions: tensor([0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,
        0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])
Test Accuracy: 69.00%
Evaluation time: 0.07 seconds
Saved QVC classifier state to vqc_nsl_kdd_torch.pth
